{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyladies Meetup Week One: Bayes Basics\n",
    "\n",
    "### Meant for an \"after work\" live presentation\n",
    "This notebook is meant for a live presentation and may be confusing to follow without direct explanation. It also is a very quick treatment of Bayes Theorem for the mean for the \"after work exhausted professional\".\n",
    "\n",
    "If you're interested after this tutorial I would suggest reading\n",
    "\n",
    "* Bayesian Analysis with Python - Osvaldo Martin\n",
    "* Statistical Rethinking - Richard McElreath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "1. Bayesian Updating Intuition\n",
    "2. Intro to Bayesian Formula\n",
    "    * Bayes with point probabilities\n",
    "    * There's so much more than point probabilities\n",
    "3. Jumping into Deep End with PyMC3 and ArviZ\n",
    "4. Breaking things apart\n",
    "    * Priors\n",
    "    * Likelihood\n",
    "    * PPLs\n",
    "    * The Magic Inference Button \n",
    "    * Posterior plots\n",
    "3. Turning intuition into statistics using PyMC3 and ArviZ\n",
    "\n",
    "### Meant for an \"after work\" live presentation\n",
    "This notebook is meant for a live presentation and may be confusing to follow without direct explanation. It also is a very quick treatment of Bayes Theorem. For some really great intros I would suggest\n",
    "\n",
    "* Bayesian Analysis with Python - Osvaldo Martin\n",
    "* Statistical Rethinking - Richard McElreath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bayesian Updating Intuition \n",
    "\n",
    "So what is Bayesian probability theory anyway? Let's start with an exercise.\n",
    "\n",
    "I have a deck of cards here (in real life). Pretend this deck of cards is infinite, and you'll never see every single one so you can never be sure of the exact counts.\n",
    "\n",
    "1. Before seeing any cards in this deck, what's the proportion of black and red?\n",
    "2. If I draw 5 what is your belief in the proportion of red vs black in the deck?\n",
    "3. If I draw 10 what is your belief in the proportion of red vs black in the deck?\n",
    "4. How do your beliefs change as you see more cards?\n",
    "\n",
    "### For the online reader\n",
    "For those reading online, at this point I'll be\n",
    "\n",
    "1. Drawing 0 cards from a loaded deck and asking people to guess the proportion of red vs black?\n",
    "2. Drawing 5 cards and asking people to guess the proportion of red vs black?\n",
    "3. Drawing 10 cards and asking people to guess the proportion of red vs black?\n",
    "4. After seeing 10 cards how sure are you that the deck is\n",
    "    * 0% Red\n",
    "    * 100% Red\n",
    "    * 50% Red\n",
    "    * 20% Red\n",
    "    \n",
    "**Takeaways**\n",
    "* As humans the more data we see the more \"sure\" we become about the world\n",
    "* For example the more of this tutorial you see, the more you'll figure out whether you like it or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Bayes Formula in a hurry\n",
    "*Disclaimer*: I'm largely skipping over mathematical details so we can spend more time on intuition\n",
    "\n",
    "The mathematical for Bayes Formula looks like this.\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "p(\\theta \\mid y) = \\frac{ p(y \\mid \\theta)p(\\theta)}{p(y)}\n",
    "$$\n",
    "\n",
    "The idea of updating beliefs based on new data is captured in Bayes Formula. Notably we have\n",
    "\n",
    "* The prior $p(\\theta)$ which describes the belief in the world before seeing new data  \n",
    "\n",
    "* The likelihood $p(y \\mid \\theta)$ which describes the probability of the \"parameter\" given the data  \n",
    "\n",
    "* The posterior $ p(\\theta \\mid y)$ which is the updated belief after seeing the data\n",
    "\n",
    "* The base rate $ p(y) $ which is the probability of the thing happening. This term difficult or impossible to ascertain in many situations so Modern Bayesian approaches utilize mathematical tricks to avoid calculating this.\n",
    "\n",
    "* Proposed \n",
    "\n",
    "### For math nerds\n",
    "This formula is derived from a rearranging of terms for joint probabilities:\n",
    "\n",
    "$$\n",
    "P(\\theta, y) = P(\\theta)P(y | \\theta) = P(y)P(\\theta | y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point probability example of Bayes Theorem (and why I don't like them)\n",
    "Here's an example taken from a data science blog post (https://towardsdatascience.com/what-is-bayes-rule-bb6598d8a2fd) that poses the question *What is the probability of cancer given you're a smoker?*\n",
    "\n",
    "\n",
    "\n",
    "So let's assume \n",
    "$P(Cancer) = .05$ (Prior)  \n",
    "$P(Smoker) = .1$ (Base Rate)  \n",
    "$P(Smoker | Cancer) = .2)$ (Likelihood)  \n",
    "\n",
    "Once we know all that all we need to do it plug it into the formula!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cancer = .05\n",
    "p_smoker = .1\n",
    "p_smoker_given_cancer = .2\n",
    "\n",
    "p_cancer_given_smoker = (p_smoker_given_cancer * p_cancer)/p_smoker\n",
    "p_cancer_given_smoker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the probability of cancer prior to \"conditioning\" on \"smoker\" was .05, but after \"conditioning\" on smoking was .1. In other words, before I knew anything about you I estimated there was a 5% chance of getting cancer, but now I know you're a smoker I estimate there is a 10% chance of you getting cancer.\n",
    "\n",
    "### Why point probability examples confused me at first\n",
    "This \"napkin\" example of Bayes Theorem is simple and easily understood but it's not very practical. Let me illustrate with our card draw example.\n",
    "\n",
    "\n",
    "#### Estimating range of outcomes\n",
    "What do we do in non yes/no (binary) outcomes? For our card example we're interested in the proportion of red versus black cards in the deck. This isn't a yes/no question but rather a belief in a range of proportions from 0 to 1, not just the probability of a single event.\n",
    "\n",
    "#### Incorporating other factors\n",
    "Additionally what if we know something else about the patient, like sex, or whether their parents had cancer?  What if we know something that is continous like age? How do we easily incorporate this new information? With point probabilities its possible but it gets messy fairly quickly\n",
    "\n",
    "#### How did we get the point probabilities?\n",
    "Where did any of these numbers come from? How do we know the base rate, or the probability of a positive test with cancer and no cancer? In practice these numbers are typically unavailable.\n",
    "\n",
    "### How the practioners do it: Distributions, Inference Algorithms, and Probabalistic Programming Languages,\n",
    "Modern Bayesian Practioners rarely think or express Bayes Formula as shown above. Instead there's a couple of key differences.\n",
    "\n",
    "1. We think in distributions of parameters and/or outcome, not just point probablities\n",
    "    *  We want to see all shades of gray, the world isn't black and white,\n",
    "2. We use Probabalistic Programming Languages to express ourselves.\n",
    "    * We want to express complicated ideas to computers with ease. Probabilities notation doesn't let us do that\n",
    "3. We use some clever math tricks and computers to estimate our beliefs, even without seeing all possible outcomes\n",
    "    * We want estimates of the world without having to see the entire world\n",
    "    * Specifically this means we avoid calculating or estiating *p(y)* in the formula through specialized algorithms\n",
    "\n",
    "\n",
    "**Takeaways**\n",
    "* Look past the point probability examples. There's more to Bayes Formula\n",
    "* Bayesian Statisticians typically think about all outcomes, not just one outcome\n",
    "* The combination of really neat math tricks, clever programmers, and very powerful computers give modern Bayesian Statisticians a diverse toolset that simplifies the expression of \"causal models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumping into the Bayesian Programmer deep end\n",
    "Alright let's just for go it. Here's a fully built PyMC3 model with ArviZ posterior plots. Don't get wrapped up in the code at the moment, focus on the results an the intuition.  \n",
    "\n",
    "In particular pay attention to the width of the distribution, and the relative height of x values different values on the y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five observations\n",
    "Given 5 observations how sure are we of the proportion of red cards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [0, 0, 0, 1, 1]\n",
    "red_observations = sum(observations)\n",
    "total_observations = len(observations)\n",
    "print(f\"Number of Red Observations {red_observations}, Total Number of Observations {total_observations}\")\n",
    "\n",
    "with pm.Model() as five_obs_model:\n",
    "    # Prior\n",
    "    p_red = pm.Uniform(\"p_red\", 0, 1)\n",
    "    \n",
    "    # Likelihood\n",
    "    num_red = pm.Binomial(\"number_observed_red\", p=p_red, n=total_observations, observed=red_observations)\n",
    "    trace = pm.sample(draws=5000)\n",
    "    \n",
    "az.plot_posterior(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ten observations\n",
    "Given 10 observations how sure are we of the proportion of red cards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.tile([0, 0, 0, 1, 1],2)\n",
    "red_observations = sum(observations)\n",
    "total_observations = len(observations)\n",
    "print(f\"Number of Red Observations {red_observations}, Total Number of Observations {total_observations}\")\n",
    "\n",
    "with pm.Model() as ten_obs_model:\n",
    "    # Prior\n",
    "    p_red = pm.Uniform(\"p_red\", 0 ,1)\n",
    "    \n",
    "    # Likelihood\n",
    "    num_red = pm.Binomial(\"number_observed_red\", p=p_red, n=total_observations, observed=red_observations)\n",
    "    trace = pm.sample(draws=5000)\n",
    "\n",
    "az.plot_posterior(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.tile([0, 0, 0, 1, 1], 20)\n",
    "red_observations = sum(observations)\n",
    "total_observations = len(observations)\n",
    "print(f\"Number of Red Observations {red_observations}, Total Number of Observations {total_observations}\")\n",
    "\n",
    "with pm.Model() as one_hundred_obs_model:\n",
    "    # Prior\n",
    "    p_red = pm.Uniform(\"p_red\", 0 ,1)\n",
    "    \n",
    "    # Likelihood\n",
    "    num_red = pm.Binomial(\"number_observed_red\", p=p_red, n=total_observations, observed=red_observations)\n",
    "    trace = pm.sample(draws=5000)\n",
    "\n",
    "az.plot_posterior(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "* As the \"Bayes math/computer magic\" sees more data it becomes more sure\n",
    "* The sureness is represented in the Posterior Distribution showing the distribution of parameters and the relative height of each\n",
    "* PPLs, like PyMC3, are the interface to a whole world of computational bayes algorithms, plotting, diagnostics, just like scikit learn is for decision trees, linear regressions etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning how to swim in Bayes Concepts, PyMC3 and ArviZ\n",
    "Now that you've seen a whole lot of Bayesian Models/Code let's break focus on each piece individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Think hard about your past beliefs, how the data is generated, and distributions (before you write code)\n",
    "\n",
    "### Picking a prior\n",
    "Let's start with priors. Before seeing any cards how do we express our beliefs? Picking priors and defending priors is an important part the Bayesian Practioners workflow.\n",
    "\n",
    "#### I have no idea (Naive prior)\n",
    "\"I have no idea\" is called the uniform distribution. Every value of proportion of red is equally likely from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_values_of_red = np.linspace(0,1, 1000)\n",
    "belief_in_parameter = np.repeat([1], 1000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(possible_values_of_red, belief_in_parameter)\n",
    "\n",
    "# ax.axvline(.2, c=\"r\", linestyle=':')\n",
    "# ax.axvline(.5, c=\"r\", linestyle=':')\n",
    "ax.set_yticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I've seen lots of decks of cards, I'm kinda sure it's 50/50 red and black (Informed prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_values_of_red = np.linspace(0,1, 1000)\n",
    "\n",
    "# Beta distributions are great at describing probabilities!\n",
    "belief_in_parameter = stats.beta(3,3).pdf(possible_values_of_red)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(possible_values_of_red, belief_in_parameter)\n",
    "\n",
    "# ax.axvline(.2, c=\"r\", linestyle=':')\n",
    "# ax.axvline(.5, c=\"r\", linestyle=':')\n",
    "\n",
    "ax.set_yticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I've seen lots of decks of cards, I'm very sure it's 50/50 red and black (Informed prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_values_of_red = np.linspace(0,1, 1000)\n",
    "belief_in_parameter = stats.beta(15,15).pdf(possible_values_of_red)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# ax.axvline(.2, c=\"r\", linestyle=':')\n",
    "# ax.axvline(.5, c=\"r\", linestyle=':')\n",
    "\n",
    "ax.plot(possible_values_of_red, belief_in_parameter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every deck I've seen has had more red than black (Informed prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_values_of_red = np.linspace(0,1, 1000)\n",
    "belief_in_parameter = stats.beta(15,6).pdf(possible_values_of_red)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# ax.axvline(.2, c=\"r\", linestyle=':')\n",
    "# ax.axvline(.5, c=\"r\", linestyle=':')\n",
    "\n",
    "ax.plot(possible_values_of_red, belief_in_parameter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "* Priors encode the belief before data from current experiment is seen\n",
    "* Priors can be distributions of beliefs, not just one belief\n",
    "* Choice of prior is a powerful tool in the Bayesian Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood: How was my data generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to define a likelihood function. There's a rigorous math definition for likelihood, but the way we'll think about it is \"How is your data generated?\"\n",
    "\n",
    "\n",
    "In our case the Binomial distribution describes \"If there's only two possibilities for each card draw, what is the number of reds we'll get with $p\\_red$ probability\".  \n",
    "\n",
    "* $p\\_red$ = 1 means you think every single card is red, so its only possible to draw red cards.\n",
    "* $p\\_red$ = 0 means you think no card is red, so its not possible to draw any red card. \n",
    "* $p\\_red$ =.4 means you think 4 out of 10 cards are red, so some draws will be red, but most will be black. \n",
    "    \n",
    "If I were estimating peoples heights I would use the Normal distribution because I know the data is continuous and likely symmetrical\n",
    "\n",
    "If I were estimating how many people arrive at a grocery store line I would use the Poisson distribution, because that distribution exists only on natural numbers (positive integers)' and it's impossible to get -1, or 2.5 customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Expressing model using a Probabalistic Programming Language\n",
    "We've seen the following data, how can estimate our belief in proportion of red?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A value of 0 signifies a black card observation, a value of 1 signifies a red card observation\n",
    "observations = [0, 0, 1, 0, 1]\n",
    "red_observations = sum(observations)\n",
    "total_observations = len(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap our choice of prior and likelihood\n",
    "\n",
    "We are trying to estimate $\\theta$ where\n",
    "$$\\theta = \\text{Proportion of red cards}$$\n",
    "\n",
    "Your model is formulated as follows, the top line being our prior belief before seeing any data, and the bottom line being our likelihood.\n",
    "\n",
    "$$ \n",
    "\\theta \\sim \\operatorname{Uniform}(0,1) \\\\\n",
    "p_{\\text{red_cards}} \\sim \\operatorname{Binom}(\\theta, N)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's express this again in the \"special\" PyMC3 language. The two lines should look vaguely familiar as the prior and likelihood.\n",
    "\n",
    "Note a couple of things:  \n",
    "1. In PyMC3 we \"tell\" python we're building a model with the `with` context manager\n",
    "2. We incorporate data into our likelihood with the `observed` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as five_obs_model:\n",
    "    # Prior\n",
    "    p_red = pm.Uniform(\"p_red\", 0 ,1)\n",
    "    \n",
    "    # Likelihood\n",
    "    num_red = pm.Binomial(\"number_observed_red\", p=p_red, n=total_observations, observed=red_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model definition looks great, no actual inference (belief estimation) has happened yet. To do that we hit \"the magic inference button\" as Thomas Wiecki calls it. In PyMC3 it looks like this.\n",
    "\n",
    "*Note*: We don't have to restate the whole model in PyMC3 but I'm choosing to do so for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as five_obs_model:\n",
    "    # Prior\n",
    "    p_red = pm.Uniform(\"p_red\", 0 ,1)\n",
    "    \n",
    "    # Likelihood\n",
    "    num_red = pm.Binomial(\"number_observed_red\", p=p_red, n=total_observations, observed=red_observations)\n",
    "    \n",
    "    # Add Inference line\n",
    "    trace_5 = pm.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is happening?\n",
    "\n",
    "1. We express our beliefs using PPL, in particular choosing a prior and likelihood\n",
    "2. PyMC3 in this case uses algorithm called Markov Chain Monte Carlo to estimate the most plausible distribution of parameters given the data. This distribution is called the posterior distribution and other algorithms can be used for posterior estimation.\n",
    "3. We inspect the posterior using ArviZ to see what the math is telling us about the world\n",
    "4. Feel accomplished that we used Math/Statistics/Machine Learning + Data to make an informed estimate of the world (or just this deck of cards I'm holding)\n",
    "\n",
    "### What would a Bayesian Practioner say\n",
    "We wrote down a *model* of our card draws using a *Probabalistic Programming Language*, and then using *numerical methods* to perform an *Inference* to estimate our *posterior distribution*\n",
    "\n",
    "## Important things that we are not doing in this tutorial\n",
    "* Validating MCMC convergence\n",
    "* Talking about other inference methods\n",
    "* Talking about model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn! Fake a bunch of data. Fit distribution\n",
    "A great way to understand statistical/ML algorithms is generate data, so you know the truth, then fit a model to your generated data to see what happens.\n",
    "\n",
    "This is where you get to play around. Suggestions are\n",
    "\n",
    "1. Generate a large amount (size=1000) at a p value of your choice. See if you can write a model to \"rediscover\" the parameter, using your posterior plot to find the estimated parameter\n",
    "2. Change the p value and generate a large amount again. See how the posterior plot changes\n",
    "3. Change your prior to different from a uniform to an informed prior and see what happens\n",
    "4. Challenge: Generate a normal distribution with with a mean and standard deviation of your choice. See if you can use Bayesian inference to \"find the parameters\"\n",
    "    * Hint: You'll need two priors, one for the mean and one for the standard deviation\n",
    "    * Hint: Your likelihood will need to change from Binomial because now your data generation is charecterisic of another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the \"hidden\" truth of the world. In practice you never \"observe\" this value\n",
    "proportion_of_red = .5\n",
    "\n",
    "# This is the number of data points, or draws. This is what you actually observe\n",
    "number_of_draws = 10\n",
    "observations = stats.bernoulli(p=proportion_of_red).rvs(size=number_of_draws)\n",
    "\n",
    "red_observations = sum(observations)\n",
    "total_observations = len(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as your_model:\n",
    "    # Prior\n",
    "    p_red = pm.Uniform(\"p_red\", 0 ,1)\n",
    "    \n",
    "    # Likelihood\n",
    "    num_red = pm.Binomial(\"number_observed_red\", p=p_red, n=total_observations, observed=red_observations)\n",
    "    \n",
    "    # Add Inference line\n",
    "    trace = pm.sample(5000)\n",
    "    \n",
    "az.plot_posterior(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Time: Bayesian Methods in Practice\n",
    "We'll cover\n",
    "* How to build more complicated models\n",
    "* Why uncertainty estimates are useful\n",
    "* Some examples of unique abilities in Bayesian Statistics "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
